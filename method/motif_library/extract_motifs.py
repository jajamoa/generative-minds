"""
Extract and merge commonly used causal structures from a person's context graphs.

This module scans all per-comment context graphs for a participant (generated by
`context_graph_builder.py`) and builds a simple motif library reflecting the
person's typical causal reasoning patterns.

We focus on small, commonly used directed motifs built from causal edges among
factors (stance edges are excluded from motif discovery):
  - Chain (M1): A -> B -> C
  - Fork (M2.1): A -> {B, C}
  - Collider (M3.1): {A, B} -> C

We also summarize extended fan-in/fan-out distributions:
  - Extended fork (1-to-k): nodes with out_degree >= 2
  - Extended collider (k-to-1): nodes with in_degree >= 2

Outputs include motif counts, a few example instances, cue distributions and
factor centrality summaries. The goal is to provide a compact view of a
participant's usual thinking patterns.
"""

from __future__ import annotations

import json
import logging
import os
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Tuple

import networkx as nx
import re


logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s - %(levelname)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


# ------------------------------- Data models ------------------------------- #

# Motif type catalog (kept aligned with src/phase2_motif/motif_library.py)
MOTIF_TYPES: Dict[str, str] = {
    "M1": "Chain",  # A → B → C
    "M2.1": "Basic Fork",  # A → B, A → C (1-to-2)
    "M2.2": "Extended Fork",  # A → B, A → C, A → D (1-to-3)
    "M2.3": "Large Fork",  # A → B, A → C, A → D, A → E, ... (1-to-4+)
    "M3.1": "Basic Collider",  # A → C, B → C (2-to-1)
    "M3.2": "Extended Collider",  # A → D, B → D, C → D (3-to-1)
    "M3.3": "Large Collider",  # A → E, B → E, C → E, D → E, ... (4+-to-1)
}


@dataclass
class MotifInstance:
    motif_type: str  # "M1:chain" | "M2.1:fork" | "M3.1:collider"
    nodes: Tuple[str, ...]  # node ids in a canonical order for the motif
    labels: Tuple[str, ...]  # human-readable labels for nodes
    comment_id: str
    cues: Tuple[str, ...]


# ---------------------------- Graph loading utils -------------------------- #


def _load_context_graphs(participant_dir: Path) -> List[Tuple[nx.DiGraph, str]]:
    """Load all per-comment context graphs under `<participant_dir>/context_graphs`.

    Returns list of (graph, comment_id).
    """
    graphs: List[Tuple[nx.DiGraph, str]] = []
    cg_dir = participant_dir / "context_graphs"
    if not cg_dir.exists():
        logger.warning(f"No context_graphs directory at: {cg_dir}")
        return graphs

    for sub in sorted(cg_dir.iterdir()):
        if not sub.is_dir():
            continue
        comment_id = sub.name
        file_path = sub / "context_graph.json"
        if not file_path.exists():
            continue
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            G = nx.DiGraph()
            # Add nodes
            for node in data.get("nodes", []):
                node_id = str(node.get("id"))
                attrs = {k: v for k, v in node.items() if k != "id"}
                G.add_node(node_id, **attrs)
            # Add edges
            for edge in data.get("edges", []):
                u = str(edge.get("source"))
                v = str(edge.get("target"))
                attrs = {k: v for k, v in edge.items() if k not in {"source", "target"}}
                G.add_edge(u, v, **attrs)
            graphs.append((G, comment_id))
        except Exception as e:
            logger.warning(f"Failed to load {file_path}: {e}")
    return graphs


def _factor_subgraph(G: nx.DiGraph) -> nx.DiGraph:
    """Return subgraph of factors."""
    H = nx.DiGraph()
    for n, attrs in G.nodes(data=True):
        if attrs.get("type") == "factor":
            H.add_node(n, **attrs)
    for u, v, attrs in G.edges(data=True):
        if u in H and v in H:
            H.add_edge(u, v, **attrs)
    return H


def _label_of(G: nx.DiGraph, node_id: str) -> str:
    return str(G.nodes[node_id].get("label", node_id))


# ------------------------------- Motif mining ------------------------------ #
def _mine_triads(H: nx.DiGraph, comment_id: str) -> List[MotifInstance]:
    """Find triad motifs (chain, fork, collider) within factor-only causal graph."""
    motifs: List[MotifInstance] = []

    # Precompute in/out neighbors for efficiency
    out_neighbors: Dict[str, set] = {n: set(H.successors(n)) for n in H.nodes()}
    in_neighbors: Dict[str, set] = {n: set(H.predecessors(n)) for n in H.nodes()}

    # Chain: A->B->C (length-2 path). We keep it even if A->C also exists.
    for b in H.nodes():
        for a in in_neighbors[b]:
            for c in out_neighbors[b]:
                if a == c:
                    continue
                nodes = (a, b, c)
                labels = (_label_of(H, a), _label_of(H, b), _label_of(H, c))
                cues = (
                    str(H[a][b].get("cue", "")),
                    str(H[b][c].get("cue", "")),
                )
                motifs.append(
                    MotifInstance(
                        motif_type="M1:chain",
                        nodes=nodes,
                        labels=labels,
                        comment_id=comment_id,
                        cues=cues,
                    )
                )

    # Fork: A->B and A->C
    for a in H.nodes():
        succs = list(out_neighbors[a])
        if len(succs) < 2:
            continue
        for i in range(len(succs)):
            for j in range(i + 1, len(succs)):
                b, c = succs[i], succs[j]
                nodes = (a, b, c)
                labels = (_label_of(H, a), _label_of(H, b), _label_of(H, c))
                cues = (str(H[a][b].get("cue", "")), str(H[a][c].get("cue", "")))
                motifs.append(
                    MotifInstance(
                        motif_type="M2.1:fork",
                        nodes=nodes,
                        labels=labels,
                        comment_id=comment_id,
                        cues=cues,
                    )
                )

    # Collider: A->C and B->C
    for c in H.nodes():
        preds = list(in_neighbors[c])
        if len(preds) < 2:
            continue
        for i in range(len(preds)):
            for j in range(i + 1, len(preds)):
                a, b = preds[i], preds[j]
                nodes = (a, b, c)
                labels = (_label_of(H, a), _label_of(H, b), _label_of(H, c))
                cues = (str(H[a][c].get("cue", "")), str(H[b][c].get("cue", "")))
                motifs.append(
                    MotifInstance(
                        motif_type="M3.1:collider",
                        nodes=nodes,
                        labels=labels,
                        comment_id=comment_id,
                        cues=cues,
                    )
                )

    return motifs


def _mine_extended(H: nx.DiGraph, comment_id: str) -> List[MotifInstance]:
    """Find extended fork/collider motifs of larger arity.

    - M2.2: Extended Fork (1-to-3) → center has out_degree==3
    - M2.3: Large Fork (1-to-4+) → center has out_degree>=4
    - M3.2: Extended Collider (3-to-1) → center has in_degree==3
    - M3.3: Large Collider (4+-to-1) → center has in_degree>=4
    """
    motifs: List[MotifInstance] = []

    # Precompute neighbors
    out_neighbors: Dict[str, List[str]] = {n: list(H.successors(n)) for n in H.nodes()}
    in_neighbors: Dict[str, List[str]] = {n: list(H.predecessors(n)) for n in H.nodes()}

    # Fork families (1-to-k)
    for a, succs in out_neighbors.items():
        k = len(succs)
        if k < 3:
            continue
        # Sort children to stabilize example ordering
        succs_sorted = sorted(succs)
        labels = tuple(_label_of(H, n) for n in (a, *succs_sorted))
        cues = tuple(str(H[a][b].get("cue", "")) for b in succs_sorted)
        if k == 3:
            motifs.append(
                MotifInstance(
                    motif_type="M2.2:extended_fork",
                    nodes=(a, *tuple(succs_sorted)),
                    labels=labels,
                    comment_id=comment_id,
                    cues=cues,
                )
            )
        if k >= 4:
            motifs.append(
                MotifInstance(
                    motif_type="M2.3:large_fork",
                    nodes=(a, *tuple(succs_sorted)),
                    labels=labels,
                    comment_id=comment_id,
                    cues=cues,
                )
            )

    # Collider families (k-to-1)
    for c, preds in in_neighbors.items():
        k = len(preds)
        if k < 3:
            continue
        preds_sorted = sorted(preds)
        labels = tuple(_label_of(H, n) for n in (*preds_sorted, c))
        cues = tuple(str(H[a][c].get("cue", "")) for a in preds_sorted)
        if k == 3:
            motifs.append(
                MotifInstance(
                    motif_type="M3.2:extended_collider",
                    nodes=(*tuple(preds_sorted), c),
                    labels=labels,
                    comment_id=comment_id,
                    cues=cues,
                )
            )
        if k >= 4:
            motifs.append(
                MotifInstance(
                    motif_type="M3.3:large_collider",
                    nodes=(*tuple(preds_sorted), c),
                    labels=labels,
                    comment_id=comment_id,
                    cues=cues,
                )
            )

    return motifs


def _extended_fan_stats(H: nx.DiGraph) -> Tuple[Counter, Counter]:
    """Return histograms of fan-out and fan-in for nodes with degree >= 2.

    fan_out[k] = number of nodes with out_degree == k for k>=2
    fan_in[k] = number of nodes with in_degree == k for k>=2
    """
    fan_out: Counter = Counter()
    fan_in: Counter = Counter()
    for n in H.nodes():
        od = H.out_degree(n)
        idg = H.in_degree(n)
        if od >= 2:
            fan_out[od] += 1
        if idg >= 2:
            fan_in[idg] += 1
    return fan_out, fan_in


# ------------------------------- Aggregations ------------------------------ #
def _dedupe_motifs(motifs: Iterable[MotifInstance]) -> List[MotifInstance]:
    """Deduplicate motifs by canonical keys (type + sorted node set + comment_id)."""
    seen: set = set()
    unique: List[MotifInstance] = []
    for m in motifs:
        key = (m.motif_type, tuple(sorted(m.nodes)), m.comment_id)
        if key in seen:
            continue
        seen.add(key)
        unique.append(m)
    return unique


def _collect_cue_distribution(G: nx.DiGraph) -> Counter:
    cues: Counter = Counter()
    for _u, _v, data in G.edges(data=True):
        if data.get("relation") == "causal":
            cue = str(data.get("cue", "")).strip().lower()
            if cue:
                cues[cue] += 1
    return cues


def _factor_roles(G: nx.DiGraph) -> Dict[str, Dict[str, int]]:
    """Return role counts per factor across the graph (source/sink/collector/distributor)."""
    roles: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    H = _factor_subgraph(G)
    for n in H.nodes():
        indeg = H.in_degree(n)
        outdeg = H.out_degree(n)
        if indeg == 0 and outdeg > 0:
            roles[n]["source"] += 1
        if outdeg == 0 and indeg > 0:
            roles[n]["sink"] += 1
        if indeg > 1:
            roles[n]["collector"] += 1
        if outdeg > 1:
            roles[n]["distributor"] += 1
    return roles


def _merge_role_counts(
    a: Dict[str, Dict[str, int]], b: Dict[str, Dict[str, int]]
) -> Dict[str, Dict[str, int]]:
    out: Dict[str, Dict[str, int]] = defaultdict(lambda: defaultdict(int))
    for src in (a, b):
        for node, role_counts in src.items():
            for role, cnt in role_counts.items():
                out[node][role] += cnt
    return out


def _top_factors_by_role(
    role_counts: Dict[str, Dict[str, int]],
    node_labels: Dict[str, str],
    role: str,
    top_k: int = 10,
) -> List[Dict[str, Any]]:
    entries: List[Tuple[str, int]] = [
        (n, rc.get(role, 0)) for n, rc in role_counts.items() if rc.get(role, 0) > 0
    ]
    entries.sort(key=lambda x: (-x[1], node_labels.get(x[0], x[0])))
    return [
        {"node_id": n, "label": node_labels.get(n, n), "count": c}
        for n, c in entries[:top_k]
    ]


# --------------------------------- Driver --------------------------------- #


def extract_person_motifs(
    participant_dir: str,
    output_dir: Optional[str] = None,
    max_examples_per_type: int = 20,
) -> Dict[str, Any]:
    """Build a simple motif library for a participant.

    Args:
        participant_dir: Path to the participant's intermediate directory, which
            contains a `context_graphs/` subdirectory with per-comment graphs.
        output_dir: Directory to save outputs. Default:
            `<participant_dir>/motif_mining`.
        max_examples_per_type: Max motif instances to include per type.

    Returns a summary dictionary. Files are also written to output_dir.
    """
    pdir = Path(participant_dir)
    out_dir = Path(output_dir) if output_dir else (pdir / "motif_library")
    out_dir.mkdir(parents=True, exist_ok=True)
    participant_name = pdir.name
    safe_name = (
        re.sub(r"[^A-Za-z0-9._-]+", "_", participant_name).strip("_") or "participant"
    )

    graphs = _load_context_graphs(pdir)
    if not graphs:
        logger.info("No context graphs found; nothing to extract.")
        return {"motifs": {}, "stats": {}}

    # Aggregate data
    all_motifs: List[MotifInstance] = []
    overall_cues: Counter = Counter()
    merged_role_counts: Dict[str, Dict[str, int]] = defaultdict(
        lambda: defaultdict(int)
    )
    node_labels_global: Dict[str, str] = {}

    # Merge across comments
    for G, comment_id in graphs:
        # Track labels for reporting
        for n, data in G.nodes(data=True):
            node_labels_global[n] = str(data.get("label", n))

        # Cue distribution (causal edges only)
        overall_cues.update(_collect_cue_distribution(G))

        # Role counts for factor nodes
        rc = _factor_roles(G)
        merged_role_counts = _merge_role_counts(merged_role_counts, rc)

        # Motifs from factor-only causal subgraph
        H = _factor_subgraph(G)
        motifs = _mine_triads(H, comment_id)
        motifs_ext = _mine_extended(H, comment_id)
        all_motifs.extend(motifs)
        all_motifs.extend(motifs_ext)

    # Deduplicate motif instances per comment
    all_motifs = _dedupe_motifs(all_motifs)

    # Group and count
    motif_groups: Dict[str, List[MotifInstance]] = defaultdict(list)
    for m in all_motifs:
        motif_groups[m.motif_type].append(m)

    group_counts = {k: len(v) for k, v in motif_groups.items()}

    # Collect extended fan-in/out histograms across all graphs merged
    # Build a single merged factor-only graph to estimate extended patterns
    merged_factor = nx.DiGraph()
    for G, _cid in graphs:
        H = _factor_subgraph(G)
        merged_factor.add_nodes_from(H.nodes(data=True))
        merged_factor.add_edges_from(H.edges(data=True))
    fan_out_hist, fan_in_hist = _extended_fan_stats(merged_factor)

    # Select examples per type
    motif_examples: Dict[str, List[Dict[str, Any]]] = {}
    for mtype, items in motif_groups.items():
        examples = []
        for m in items[:max_examples_per_type]:
            examples.append(
                {
                    "comment_id": m.comment_id,
                    "nodes": list(m.nodes),
                    "labels": list(m.labels),
                    "cues": [c for c in m.cues if c],
                }
            )
        motif_examples[mtype] = examples

    # Top factors by roles
    sources_top = _top_factors_by_role(merged_role_counts, node_labels_global, "source")
    sinks_top = _top_factors_by_role(merged_role_counts, node_labels_global, "sink")
    collectors_top = _top_factors_by_role(
        merged_role_counts, node_labels_global, "collector"
    )
    distributors_top = _top_factors_by_role(
        merged_role_counts, node_labels_global, "distributor"
    )

    summary: Dict[str, Any] = {
        "motif_counts": group_counts,
        "motif_examples": motif_examples,
        "cue_distribution": dict(overall_cues),
        "extended_forks_out_degree_hist": dict(fan_out_hist),
        "extended_colliders_in_degree_hist": dict(fan_in_hist),
        "top_factors": {
            "sources": sources_top,
            "sinks": sinks_top,
            "collectors": collectors_top,
            "distributors": distributors_top,
        },
        "total_comments": len(graphs),
        "total_motif_instances": len(all_motifs),
    }

    # Persist
    out_summary = out_dir / f"{safe_name}_motif_library.json"
    with open(out_summary, "w", encoding="utf-8") as f:
        json.dump(summary, f, indent=2, ensure_ascii=False)

    logger.info(f"Wrote motif library to: {out_summary}")

    return summary


def main() -> None:
    import argparse

    parser = argparse.ArgumentParser(
        description="Extract common causal motifs from a participant's context graphs"
    )
    parser.add_argument(
        "--participant_dir",
        required=True,
        help="Path to participant intermediate dir containing context_graphs/",
    )
    parser.add_argument(
        "--output_dir",
        default=None,
        help="Directory to write outputs (default: <participant_dir>/motif_mining)",
    )
    parser.add_argument(
        "--examples",
        type=int,
        default=100,
        help="Max examples to save per motif type",
    )

    args = parser.parse_args()
    extract_person_motifs(
        participant_dir=args.participant_dir,
        output_dir=args.output_dir,
        max_examples_per_type=args.examples,
    )


if __name__ == "__main__":
    main()
